diff --git a/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java b/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java
index 162a7f8258..38219df7dd 100644
--- a/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java
+++ b/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java
@@ -25,9 +25,11 @@ import java.net.MalformedURLException;
 import java.util.ArrayList;
 import java.util.Calendar;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.UUID;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.LinkedBlockingQueue;
@@ -35,8 +37,10 @@ import java.util.concurrent.TimeUnit;
 
 import org.apache.ambari.logfeeder.input.InputMarker;
 import org.apache.ambari.logfeeder.util.DateUtil;
+import org.apache.ambari.logfeeder.util.LogFeederPropertiesUtil;
 import org.apache.ambari.logfeeder.util.LogFeederUtil;
-import org.apache.ambari.logsearch.config.api.LogSearchPropertyDescription;
+import org.apache.ambari.logsearch.config.api.model.outputconfig.OutputProperties;
+import org.apache.ambari.logsearch.config.api.model.outputconfig.OutputSolrProperties;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.log4j.Level;
 import org.apache.log4j.Logger;
@@ -44,53 +48,34 @@ import org.apache.solr.client.solrj.SolrClient;
 import org.apache.solr.client.solrj.SolrServerException;
 import org.apache.solr.client.solrj.impl.CloudSolrClient;
 import org.apache.solr.client.solrj.impl.HttpClientUtil;
-import org.apache.solr.client.solrj.impl.HttpSolrClient;
 import org.apache.solr.client.solrj.impl.Krb5HttpClientConfigurer;
-import org.apache.solr.client.solrj.impl.LBHttpSolrClient;
 import org.apache.solr.client.solrj.response.SolrPingResponse;
 import org.apache.solr.client.solrj.response.UpdateResponse;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.cloud.CollectionStateWatcher;
+import org.apache.solr.common.cloud.DocCollection;
 
-import static org.apache.ambari.logfeeder.util.LogFeederUtil.LOGFEEDER_PROPERTIES_FILE;
-
-public class OutputSolr extends Output {
-
-  private static final String DEFAULT_SOLR_JAAS_FILE = "/etc/security/keytabs/logsearch_solr.service.keytab";
-  @LogSearchPropertyDescription(
-    name = "logfeeder.solr.jaas.file",
-    description = "The jaas file used for solr.",
-    examples = {"/etc/ambari-logsearch-logfeeder/conf/logfeeder_jaas.conf"},
-    defaultValue = DEFAULT_SOLR_JAAS_FILE,
-    sources = {LOGFEEDER_PROPERTIES_FILE}
-  )
-  private static final String SOLR_JAAS_FILE_PROPERTY = "logfeeder.solr.jaas.file";
+public class OutputSolr extends Output implements CollectionStateWatcher {
 
   private static final Logger LOG = Logger.getLogger(OutputSolr.class);
 
-  private static final boolean DEFAULT_SOLR_KERBEROS_ENABLE = false;
-  @LogSearchPropertyDescription(
-    name = "logfeeder.solr.kerberos.enable",
-    description = "Enables using kerberos for accessing solr.",
-    examples = {"true"},
-    defaultValue = DEFAULT_SOLR_KERBEROS_ENABLE + "",
-    sources = {LOGFEEDER_PROPERTIES_FILE}
-  )
-  private static final String SOLR_KERBEROS_ENABLE_PROPERTY = "logfeeder.solr.kerberos.enable";
+  private static final int OUTPUT_PROPERTIES_WAIT_MS = 10000;
+  private static final int SHARDS_WAIT_MS = 10000;
 
   private static final int DEFAULT_MAX_BUFFER_SIZE = 5000;
   private static final int DEFAULT_MAX_INTERVAL_MS = 3000;
-  private static final int DEFAULT_NUMBER_OF_SHARDS = 1;
-  private static final int DEFAULT_SPLIT_INTERVAL = 30;
   private static final int DEFAULT_NUMBER_OF_WORKERS = 1;
   private static final boolean DEFAULT_SKIP_LOGTIME = false;
 
   private static final int RETRY_INTERVAL = 30;
 
+  private String type;
   private String collection;
   private String splitMode;
   private int splitInterval;
-  private int numberOfShards;
+  private List<String> shards;
+  private String zkConnectString;
   private int maxIntervalMS;
   private int workers;
   private int maxBufferSize;
@@ -98,9 +83,21 @@ public class OutputSolr extends Output {
   private int lastSlotByMin = -1;
   private boolean skipLogtime = false;
 
+  private final Object propertiesLock = new Object();
+
   private BlockingQueue<OutputData> outgoingBuffer = null;
   private List<SolrWorkerThread> workerThreadList = new ArrayList<>();
 
+  @Override
+  public boolean monitorConfigChanges() {
+    return true;
+  };
+
+  @Override
+  public String getOutputType() {
+    return type;
+  }
+
   @Override
   protected String getStatMetricName() {
     return "output.solr.write_logs";
@@ -110,24 +107,34 @@ public class OutputSolr extends Output {
   protected String getWriteBytesMetricName() {
     return "output.solr.write_bytes";
   }
-
+  
   @Override
   public void init() throws Exception {
     super.init();
     initParams();
     setupSecurity();
     createOutgoingBuffer();
+    createSolrStateWatcher();
     createSolrWorkers();
   }
 
   private void initParams() throws Exception {
-    splitMode = getStringValue("splits_interval_mins", "none");
-    if (!splitMode.equalsIgnoreCase("none")) {
-      splitInterval = getIntValue("split_interval_mins", DEFAULT_SPLIT_INTERVAL);
+    type = getStringValue("type");
+    while (true) {
+      OutputSolrProperties outputSolrProperties = logSearchConfig.getOutputSolrProperties(type);
+      if (outputSolrProperties == null) {
+        LOG.info("Output solr properties for type " + type + " is not available yet.");
+        try { Thread.sleep(OUTPUT_PROPERTIES_WAIT_MS); } catch (Exception e) { LOG.warn(e); }
+      } else {
+        initPropertiesFromLogSearchConfig(outputSolrProperties, true);
+        break;
+      }
+    }
+
+    zkConnectString = getStringValue("zk_connect_string");
+    if (StringUtils.isEmpty(zkConnectString)) {
+      throw new Exception("For solr output the zk_connect_string property need to be set");
     }
-    isComputeCurrentCollection = !splitMode.equalsIgnoreCase("none");
-    
-    numberOfShards = getIntValue("number_of_shards", DEFAULT_NUMBER_OF_SHARDS);
 
     skipLogtime = getBooleanValue("skip_logtime", DEFAULT_SKIP_LOGTIME);
 
@@ -140,22 +147,39 @@ public class OutputSolr extends Output {
       maxBufferSize = 1;
     }
 
-    collection = getStringValue("collection");
-    if (StringUtils.isEmpty(collection)) {
-      throw new Exception("Collection property is mandatory");
-    }
+    LOG.info(String.format("Config: Number of workers=%d, splitMode=%s, splitInterval=%d."
+        + getShortDescription(), workers, splitMode, splitInterval));
+  }
 
-    LOG.info(String.format("Config: Number of workers=%d, splitMode=%s, splitInterval=%d, numberOfShards=%d. "
-        + getShortDescription(), workers, splitMode, splitInterval, numberOfShards));
+  @Override
+  public void outputConfigChanged(OutputProperties outputProperties) {
+    initPropertiesFromLogSearchConfig((OutputSolrProperties)outputProperties, false);
   }
 
+  private void initPropertiesFromLogSearchConfig(OutputSolrProperties outputSolrProperties, boolean init) {
+    synchronized (propertiesLock) {
+      splitMode = outputSolrProperties.getSplitIntervalMins();
+      if (!splitMode.equalsIgnoreCase("none")) {
+        splitInterval = Integer.parseInt(splitMode);
+      }
+      isComputeCurrentCollection = !splitMode.equalsIgnoreCase("none");
+
+      // collection can not be overwritten after initialization
+      if (init) {
+        collection = outputSolrProperties.getCollection();
+        if (StringUtils.isEmpty(collection)) {
+          throw new IllegalStateException("Collection property is mandatory");
+        }
+      }
+    }
+  }
 
   private void setupSecurity() {
-    String jaasFile = LogFeederUtil.getStringProperty(SOLR_JAAS_FILE_PROPERTY, DEFAULT_SOLR_JAAS_FILE);
-    boolean securityEnabled = LogFeederUtil.getBooleanProperty(SOLR_KERBEROS_ENABLE_PROPERTY, DEFAULT_SOLR_KERBEROS_ENABLE);
+    String jaasFile = LogFeederPropertiesUtil.getSolrJaasFile();
+    boolean securityEnabled = LogFeederPropertiesUtil.isSolrKerberosEnabled();
     if (securityEnabled) {
       System.setProperty("java.security.auth.login.config", jaasFile);
-      HttpClientUtil.setConfigurer(new Krb5HttpClientConfigurer());
+      HttpClientUtil.addConfigurer(new Krb5HttpClientConfigurer());
       LOG.info("setupSecurity() called for kerberos configuration, jaas file: " + jaasFile);
     }
   }
@@ -166,81 +190,73 @@ public class OutputSolr extends Output {
     outgoingBuffer = new LinkedBlockingQueue<OutputData>(bufferSize);
   }
 
-  private void createSolrWorkers() throws Exception, MalformedURLException {
-    String solrUrl = getStringValue("url");
-    String zkConnectString = getStringValue("zk_connect_string");
-    if (StringUtils.isEmpty(solrUrl) && StringUtils.isEmpty(zkConnectString)) {
-      throw new Exception("For solr output, either url or zk_connect_string property need to be set");
+  private void createSolrStateWatcher() throws Exception {
+    if ("none".equals(splitMode)) {
+      return;
+    }
+    
+    CloudSolrClient stateWatcherClient = createSolrClient();
+    stateWatcherClient.registerCollectionStateWatcher(collection, this);
+    while (true) {
+      if (shards == null) {
+        LOG.info("Shards are not available yet, waiting ...");
+        try { Thread.sleep(SHARDS_WAIT_MS); } catch (Exception e) { LOG.warn(e); }
+      } else {
+        break;
+      }
     }
+  }
+
+  @Override
+  public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {
+    synchronized (propertiesLock) {
+      if (collectionState != null) {
+        List<String> shards = new ArrayList<>(collectionState.getSlicesMap().keySet());
+        Collections.sort(shards);
+        this.shards = shards;
+      }
+    }
+    return false;
+  }
 
+  private void createSolrWorkers() throws Exception, MalformedURLException {
     for (int count = 0; count < workers; count++) {
-      SolrClient solrClient = getSolrClient(solrUrl, zkConnectString, count);
+      CloudSolrClient solrClient = getSolrClient(count);
       createSolrWorkerThread(count, solrClient);
     }
   }
 
-  SolrClient getSolrClient(String solrUrl, String zkConnectString, int count) throws Exception, MalformedURLException {
-    SolrClient solrClient = createSolrClient(solrUrl, zkConnectString);
-    pingSolr(solrUrl, zkConnectString, count, solrClient);
-
-    return solrClient;
-  }
+  CloudSolrClient getSolrClient(int count) throws Exception, MalformedURLException {
+    CloudSolrClient solrClient = createSolrClient();
+    pingSolr(count, solrClient);
 
-  private SolrClient createSolrClient(String solrUrl, String zkConnectString) throws Exception, MalformedURLException {
-    SolrClient solrClient;
-    if (zkConnectString != null) {
-      solrClient = createCloudSolrClient(zkConnectString);
-    } else {
-      solrClient = createHttpSolarClient(solrUrl);
-    }
     return solrClient;
   }
 
-  private SolrClient createCloudSolrClient(String zkConnectString) throws Exception {
+  private CloudSolrClient createSolrClient() throws Exception {
     LOG.info("Using zookeepr. zkConnectString=" + zkConnectString);
-    collection = getStringValue("collection");
-    if (StringUtils.isEmpty(collection)) {
-      throw new Exception("For solr cloud property collection is mandatory");
-    }
     LOG.info("Using collection=" + collection);
 
-    CloudSolrClient solrClient = new CloudSolrClient(zkConnectString);
+    CloudSolrClient solrClient = new CloudSolrClient.Builder().withZkHost(zkConnectString).build();
     solrClient.setDefaultCollection(collection);
     return solrClient;
   }
 
-  private SolrClient createHttpSolarClient(String solrUrl) throws MalformedURLException {
-    String[] solrUrls = StringUtils.split(solrUrl, ",");
-    if (solrUrls.length == 1) {
-      LOG.info("Using SolrURL=" + solrUrl);
-      return new HttpSolrClient(solrUrl + "/" + collection);
-    } else {
-      LOG.info("Using load balance solr client. solrUrls=" + solrUrl);
-      LOG.info("Initial URL for LB solr=" + solrUrls[0] + "/" + collection);
-      LBHttpSolrClient lbSolrClient = new LBHttpSolrClient(solrUrls[0] + "/" + collection);
-      for (int i = 1; i < solrUrls.length; i++) {
-        LOG.info("Adding URL for LB solr=" + solrUrls[i] + "/" + collection);
-        lbSolrClient.addSolrServer(solrUrls[i] + "/" + collection);
-      }
-      return lbSolrClient;
-    }
-  }
-
-  private void pingSolr(String solrUrl, String zkConnectString, int count, SolrClient solrClient) {
+  private void pingSolr(int count, CloudSolrClient solrClient) {
     try {
-      LOG.info("Pinging Solr server. zkConnectString=" + zkConnectString + ", urls=" + solrUrl);
+      LOG.info("Pinging Solr server. zkConnectString=" + zkConnectString);
       SolrPingResponse response = solrClient.ping();
       if (response.getStatus() == 0) {
         LOG.info("Ping to Solr server is successful for worker=" + count);
       } else {
         LOG.warn(
-            String.format("Ping to Solr server failed. It would check again. worker=%d, solrUrl=%s, zkConnectString=%s, " +
-                "collection=%s, response=%s", count, solrUrl, zkConnectString, collection, response));
+            String.format("Ping to Solr server failed. It would check again. worker=%d, zkConnectString=%s, collection=%s, " +
+                "response=%s", count, zkConnectString, collection, response));
       }
     } catch (Throwable t) {
       LOG.warn(String.format(
-          "Ping to Solr server failed. It would check again. worker=%d, " + "solrUrl=%s, zkConnectString=%s, collection=%s",
-          count, solrUrl, zkConnectString, collection), t);
+          "Ping to Solr server failed. It would check again. worker=%d, zkConnectString=%s, collection=%s", count,
+          zkConnectString, collection), t);
     }
   }
 
@@ -359,21 +375,22 @@ public class OutputSolr extends Output {
           if (outputData != null) {
             createSolrDocument(outputData);
           } else {
-            if (isDrain() && outgoingBuffer.size() == 0) {
+            if (isDrain() && outgoingBuffer.isEmpty()) {
               break;
             }
           }
 
-          if (localBuffer.size() > 0 && ((outputData == null && isDrain()) ||
-              (nextDispatchDuration <= 0 || localBuffer.size() >= maxBufferSize))) {
+          if (!localBuffer.isEmpty() &&
+              (outputData == null && isDrain() || nextDispatchDuration <= 0 || localBuffer.size() >= maxBufferSize)
+             ) {
             boolean response = sendToSolr(outputData);
-            if( isDrain() && !response) {
+            if (isDrain() && !response) {
               //Since sending to Solr response failed and it is in draining mode, let's break;
               LOG.warn("In drain mode and sending to Solr failed. So exiting. output=" + getShortDescription());
               break;
             }
           }
-          if (localBuffer.size() == 0) {
+          if (localBuffer.isEmpty()) {
             //If localBuffer is empty, then reset the timer
             lastDispatchTime = currTimeMS;
           }
@@ -402,9 +419,11 @@ public class OutputSolr extends Output {
       boolean result = false;
       while (!isDrain()) {
         try {
-          if (isComputeCurrentCollection) {
-            // Compute the current router value
-            addRouterField();
+          synchronized (propertiesLock) {
+            if (isComputeCurrentCollection) {
+              // Compute the current router value
+              addRouterField();
+            }
           }
           addToSolr(outputData);
           resetLocalBuffer();
@@ -468,9 +487,9 @@ public class OutputSolr extends Output {
       int currMin = cal.get(Calendar.MINUTE);
 
       int minOfWeek = (weekDay - 1) * 24 * 60 + currHour * 60 + currMin;
-      int slotByMin = minOfWeek / splitInterval % numberOfShards;
+      int slotByMin = minOfWeek / splitInterval % shards.size();
 
-      String shard = "shard" + slotByMin;
+      String shard = shards.get(slotByMin);
 
       if (lastSlotByMin != slotByMin) {
         LOG.info("Switching to shard " + shard + ", output=" + getShortDescription());